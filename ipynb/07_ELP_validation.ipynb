{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Early-to-late Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from albumentations import CenterCrop, Compose, Normalize\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (average_precision_score, balanced_accuracy_score,\n",
    "                             matthews_corrcoef, roc_auc_score)\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "\n",
    "from toxreprcnn.dataset import ToxReprCNNDataset\n",
    "from toxreprcnn.data_split import RepeatedStratifiedGroupKFold\n",
    "from toxreprcnn.model import EffnetB4ModelMO, FrozenEffnetB4ModelMO\n",
    "from toxreprcnn.utils import fix_seed\n",
    "\n",
    "root = \"..\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3457: DtypeWarning: Columns (6) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "info = pd.read_csv(f\"{root}/data/TGGATEs/processed/info.csv\")\n",
    "train = pd.read_csv(f\"{root}/data/TGGATEs/processed/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_info = info[~info[\"COMPOUND_NAME\"].isin(train[\"COMPOUND_NAME\"])]\n",
    "test_info = test_info[(test_info[\"DOSE\"]>0) & (test_info[\"SACRI_PERIOD\"].isin([\"4 day\", \"8 day\", \"15 day\", \"29 day\"]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1588/1588 [00:00<00:00, 5560.73it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "141193"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tiles = []\n",
    "\n",
    "for f in tqdm(test_info[\"FILE\"].values):\n",
    "    for p in glob(f\"/mnt/local/extHDD1/TGGATE/tiles/{f}/*.tiff\"):\n",
    "        test_tiles.append(p)\n",
    "len(test_tiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 123\n",
    "fix_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 512\n",
    "\n",
    "vl_transform = Compose([CenterCrop(image_size, image_size), Normalize(), ToTensorV2()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = ToxReprCNNDataset(\n",
    "    test_tiles, [0]*len(test_tiles), transform=vl_transform\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=32,\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "model_control = timm.create_model(\"tf_efficientnet_b4_ns\", pretrained=True, num_classes=0)\n",
    "model_control.eval()\n",
    "model_control.to(\"cuda\")\n",
    "\n",
    "ft_list = [None]*8\n",
    "\n",
    "models_multiseed = []\n",
    "for seed in range(123,128):\n",
    "    save_dir = f\"{root}/outputs/TGGATEs_model_seed{seed}\"\n",
    "    models = [FrozenEffnetB4ModelMO(i, len(ft_list)) for i in range(8)] + [EffnetB4ModelMO(num_classes=len(ft_list))]\n",
    "    for i, model in enumerate(models):\n",
    "        if i <= 7:\n",
    "            model.load_state_dict(\n",
    "                torch.load(f\"{save_dir}/{i}/effnetb4_freeze{i}_fold0_best_loss.pth\")\n",
    "            )\n",
    "            model.classifier = nn.Identity()\n",
    "        else:\n",
    "            model.model.load_state_dict(\n",
    "                torch.load(f\"{save_dir}/{i}/effnetb4_freeze{i}_fold0_best_loss.pth\")\n",
    "            )\n",
    "            model.model.classifier = nn.Identity()\n",
    "        model.to(\"cuda\")\n",
    "        model.eval()\n",
    "    models_multiseed.append(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4413/4413 [8:02:57<00:00,  6.57s/it]  \n"
     ]
    }
   ],
   "source": [
    "features_control = []\n",
    "features = [[[[] for k in range(9)] for i in range(9)] for seed in range(5)]\n",
    "with torch.no_grad():\n",
    "    for im, _ in tqdm(test_loader):\n",
    "        im = im.to(\"cuda\")\n",
    "        outputs = model_control(im)\n",
    "        features_control.append(outputs.to(\"cpu\").numpy())\n",
    "        for seed in range(5):\n",
    "            for j in range(9):\n",
    "                outputs = models_multiseed[seed][j](im)\n",
    "                for k, f in enumerate(outputs):\n",
    "                    features[seed][j][k].append(f.to(\"cpu\").numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "for seed in range(5):\n",
    "    print(seed)\n",
    "    with open(f\"/mnt/local/extHDD2/data/TGGATE/230310prognosis_features_seed{123+seed}.pickle\", \"wb\") as f:\n",
    "        pickle.dump({\"features\" : features[seed],\n",
    "                    \"features_control\" : features_control,\n",
    "                    \"test_tiles\" : test_tiles}, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "EOFError",
     "evalue": "Ran out of input",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEOFError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_622194/870560736.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../../outputs/230310prognosis_features_multiseed.pickle\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mEOFError\u001b[0m: Ran out of input"
     ]
    }
   ],
   "source": [
    "with open(\"../../outputs/230310prognosis_features_multiseed.pickle\", \"rb\") as f:\n",
    "    features = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = features\n",
    "features = data[\"features\"]\n",
    "features_control = data[\"features_control\"]\n",
    "test_tiles = data[\"test_tiles\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_info = test_info[test_info[\"SACRI_PERIOD\"].isin([\"4 day\", \"8 day\"])]\n",
    "late_info = test_info[test_info[\"SACRI_PERIOD\"].isin([\"15 day\", \"29 day\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_all_list = ['Accumulation, foam cell', 'Adenoma, hepatocellular',\n",
    "       'Alteration, cytoplasmic', 'Alteration, nuclear',\n",
    "       'Altered hepatocellular foci', 'Anisonucleosis', 'Atrophy',\n",
    "       'Atypia, nuclear', 'Bacterium', 'Cellular foci',\n",
    "       'Cellular infiltration', 'Cellular infiltration, mononuclear cell',\n",
    "       'Cellular infiltration, neutrophil', 'Change, acidophilic',\n",
    "       'Change, basophilic', 'Change, eosinophilic', 'Congestion', 'Cyst',\n",
    "       'DEAD', 'Degeneration', 'Degeneration, acidophilic, eosinophilic',\n",
    "       'Degeneration, fatty', 'Degeneration, granular',\n",
    "       'Degeneration, granular, eosinophilic', 'Degeneration, hydropic',\n",
    "       'Degeneration, vacuolar', 'Deposit, glycogen', 'Deposit, hemosiderin',\n",
    "       'Deposit, lipid', 'Deposit, pigment', 'Dilatation', 'Disarrangement',\n",
    "       'Ectopic tissue', 'Edema', 'Fibrosis', 'Giant cell', 'Granuloma',\n",
    "       'Ground glass appearance', 'Hematopoiesis, extramedullary',\n",
    "       'Hemorrhage', 'Hyperplasia', 'Hypertrophy',\n",
    "       'Inclusion body, intracytoplasmic', 'Increased mitosis', 'Inflammation',\n",
    "       'Inflammation, foreign body', 'Inflammation, suppurative', 'Lesion,NOS',\n",
    "       'Microgranuloma', 'Mineralization', 'Necrosis', 'Necrosis, fibrinoid',\n",
    "       'Nodule, hepatodiaphragmatic', 'Phagocytosis', 'Proliferation',\n",
    "       'Proliferation, Kupffer cell', 'Proliferation, bile duct',\n",
    "       'Proliferation, oval cell', 'Pyknosis', 'Scar', 'Single cell necrosis',\n",
    "       'Swelling', 'Thrombus', 'Vacuolization, cytoplasmic',\n",
    "       'Vacuolization, nuclear']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "compound_label = {}\n",
    "for com in late_info[\"COMPOUND_NAME\"].unique():\n",
    "    compound_label[com] = late_info[late_info[\"COMPOUND_NAME\"] == com][ft_all_list].mean().to_numpy() > 0\n",
    "\n",
    "early_label = np.array([compound_label[com] for com in early_info[\"COMPOUND_NAME\"].to_numpy()])\n",
    "\n",
    "for i, ft in enumerate(ft_all_list):\n",
    "    early_info[\"early_\" + ft] = early_label[:, i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(gt, pr):\n",
    "    return [\n",
    "        roc_auc_score(gt >= 0.5, pr),\n",
    "        matthews_corrcoef(gt >= 0.5, pr >= 0.5),\n",
    "        balanced_accuracy_score(gt >= 0.5, pr >= 0.5),\n",
    "        (average_precision_score(gt >= 0.5, pr) + average_precision_score(gt < 0.5, -pr))/2,\n",
    "    ]\n",
    "\n",
    "\n",
    "def prognosis_test(features, test_tiles):\n",
    "    ret = []\n",
    "    if features is not None:\n",
    "        wsi = [tile.split(\"/\")[-2] for tile in test_tiles]\n",
    "        df = pd.DataFrame(features)\n",
    "        df[\"FILE\"] = wsi\n",
    "        late_features = pd.merge(late_info, df, on=\"FILE\", how=\"inner\")\n",
    "        late_features = late_features.groupby(\"FILE\").mean()\n",
    "        X_train = late_features[range(features.shape[1])].to_numpy()\n",
    "        early_features = pd.merge(early_info, df, on=\"FILE\", how=\"inner\")\n",
    "        early_features = early_features.groupby(\"FILE\").mean()\n",
    "        X_test = early_features[range(features.shape[1])].to_numpy()\n",
    "        lr_list = []\n",
    "        for ft in ft_all_list:\n",
    "            y_train = late_features[ft].to_numpy()\n",
    "            if y_train.sum() == 0:\n",
    "                ret.append([np.nan]*4)\n",
    "                lr_list.append(None)\n",
    "                continue\n",
    "            lr = LogisticRegression(max_iter=10000)\n",
    "            lr.fit(X_train, y_train)\n",
    "            y_test = early_features[\"early_\" + ft].to_numpy()\n",
    "            if y_test.sum() == 0:\n",
    "                ret.append([np.nan]*4)\n",
    "                lr_list.append(None)\n",
    "                continue\n",
    "            y_preds = lr.predict_proba(X_test)[:, 1]\n",
    "            ret.append(eval(y_test, y_preds))\n",
    "            lr_list.append(lr)\n",
    "        return ret, lr_list\n",
    "    else:\n",
    "        for ft in ft_all_list:\n",
    "            y_test = early_info[\"early_\" + ft].to_numpy(dtype=np.int64)\n",
    "            if y_test.sum() == 0:\n",
    "                ret.append([np.nan]*4)\n",
    "                continue\n",
    "            y_preds = early_info[ft].to_numpy(dtype=np.int64)\n",
    "            ret.append(eval(y_test, y_preds))\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [03:46<00:24, 24.90s/it]/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "100%|██████████| 10/10 [04:13<00:00, 25.37s/it]\n",
      "100%|██████████| 10/10 [04:12<00:00, 25.23s/it]\n",
      "100%|██████████| 10/10 [04:11<00:00, 25.14s/it]\n",
      "100%|██████████| 10/10 [04:07<00:00, 24.76s/it]\n",
      "100%|██████████| 10/10 [04:08<00:00, 24.89s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "result = [[[0 for j in range(9)] for i in range(10)] for seed in range(5)]\n",
    "lrs = [[[0 for j in range(9)] for i in range(10)] for seed in range(5)]\n",
    "for seed in range(5):\n",
    "    features_temp = [[features[seed][0][i] for i in range(8)] + [features_control]] + features[seed]\n",
    "    for i in tqdm(range(10)):\n",
    "        for j in range(9):\n",
    "            result[seed][i][j], lrs[seed][i][j] = prognosis_test(np.concatenate(features_temp[i][j]), test_tiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_ft = prognosis_test(None, test_tiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = np.array(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for seed in range(5):\n",
    "    model_name = [\"Control\"] + [\"Head\"] + [f\"Block {7-i}\" for i in range(7)] + [\"Full\"]\n",
    "    layer_name = [\"Stem\"] + [f\"Block {i+1}\" for i in range(7)] + [\"Head\"]\n",
    "    rec = []\n",
    "    for i in range(10):\n",
    "        for j in range(9):\n",
    "            for k in range(len(ft_all_list)):\n",
    "                if np.isnan(result[seed][i][j][k][0]):\n",
    "                    continue\n",
    "                rec.append([model_name[i], layer_name[j], ft_all_list[k]] + [result[seed][i][j][k][0], result[seed][i][j][k][3]])\n",
    "    for k in range(len(ft_all_list)):\n",
    "        if np.isnan(result_ft[k][0]):\n",
    "            continue\n",
    "        rec.append([\"Pathological Findings\", \"-\", ft_all_list[k]] + [result_ft[k][0], result_ft[k][3]])\n",
    "    raw_df = pd.DataFrame(rec, columns=[\"model\", \"layer\", \"finding type\", \"AUROC\", \"AP\"])\n",
    "    raw_df.to_csv(f\"{root}/outputs/prognosis_result_{seed+123}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
