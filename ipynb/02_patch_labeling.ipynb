{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Patch labeling\n",
    "\n",
    "1. Weakly Supervised Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/ToxRepresentatonCNN/ipynb\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "root = \"..\"\n",
    "sys.path.append(root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import os\n",
    "from toxreprcnn.train import train_loop\n",
    "from toxreprcnn.utils import Logger\n",
    "from toxreprcnn.dataset import load_features\n",
    "from toxreprcnn.data_split import Fold, CrossValidation\n",
    "from toxreprcnn.evaluate import (\n",
    "    Metrics,\n",
    "    macro_accuracy,\n",
    "    macro_auroc,\n",
    "    macro_balanced_accuracy,\n",
    "    auroc,\n",
    "    accuracy,\n",
    "    macro_r2_score,\n",
    ")\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import timm\n",
    "from toxreprcnn.utils import fix_seed\n",
    "import pandas as pd\n",
    "import albumentations as albu\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from toxreprcnn.dataset import ToxReprCNNDataset, ToxReprCNNBalancedSampler\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from sklearn.model_selection import GroupKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tile_dir = f\"{root}/TGGATEs/tiles\"\n",
    "save_path = f\"{root}/output/wsl/result\"\n",
    "random_state = 123\n",
    "model_name = \"tf_efficientnet_b4_ns\"\n",
    "feature_extraction = False\n",
    "pretrained = True\n",
    "image_size = 512\n",
    "n_tiles = 100\n",
    "num_workers = 4\n",
    "train_batch_size = 32\n",
    "valid_batch_size = 32\n",
    "learning_rate = 0.0005\n",
    "n_epochs = 20\n",
    "verbose = 20\n",
    "device = \"cuda\"\n",
    "debug = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3457: DtypeWarning: Columns (6) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "info_path = f\"{root}/data/TGGATEs/processed/info.csv\"\n",
    "info = pd.read_csv(info_path)\n",
    "\n",
    "info[\"EG\"] = 100 * info[\"EXP_ID\"] + info[\"GROUP_ID\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.benchmark = True\n",
    "random_state = 123\n",
    "fix_seed(random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gkf = GroupKFold(n_splits=5)\n",
    "\n",
    "# metrics = [\n",
    "#     Metrics(\"macro AUROC\", macro_auroc, \"+\", limit=(0, 1)),\n",
    "#     Metrics(\"macro AUROC\", macro_accuracy, \"+\", limit=(0, 1)),\n",
    "# ]\n",
    "\n",
    "\n",
    "# train_transform = [\n",
    "#     albu.RandomCrop(image_size, image_size),\n",
    "#     albu.HorizontalFlip(p=0.5),\n",
    "#     albu.VerticalFlip(p=0.5),\n",
    "#     albu.Normalize(\n",
    "#         mean=[0.485, 0.456, 0.406],\n",
    "#         std=[0.229, 0.224, 0.225],\n",
    "#     ),\n",
    "#     ToTensorV2(),\n",
    "# ]\n",
    "# train_transform = albu.Compose(train_transform)\n",
    "# valid_transform = albu.Compose(\n",
    "#     [\n",
    "#         albu.CenterCrop(image_size, image_size),\n",
    "#         albu.Normalize(\n",
    "#             mean=[0.485, 0.456, 0.406],\n",
    "#             std=[0.229, 0.224, 0.225],\n",
    "#         ),\n",
    "#         ToTensorV2(),\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# import numpy as np\n",
    "# from glob import glob\n",
    "\n",
    "# for ft in ft_list:\n",
    "#     shuffled = info.sample(frac=1, random_state=random_state)\n",
    "#     train_idx, valid_idx = next(gkf.split(shuffled[\"FILE\"], shuffled[ft], shuffled[\"EG\"]))\n",
    "#     fold = np.zeros(len(shuffled), dtype=np.int64)\n",
    "#     fold[valid_idx] = 1\n",
    "#     shuffled[\"fold\"] = fold\n",
    "\n",
    "#     train_patches = []\n",
    "#     train_labels = []\n",
    "#     for wsi, label in shuffled[shuffled[\"fold\"] == 0][[\"FILE\", ft]].to_numpy():\n",
    "#         patches = glob(f\"{tile_dir}/wsi/*\")\n",
    "#         train_patches += patches\n",
    "#         train_labels += [label]*len(patches)\n",
    "\n",
    "#     valid_patches = []\n",
    "#     valid_labels = []\n",
    "#     for wsi, label in shuffled[shuffled[\"fold\"] == 1][[\"FILE\", ft]].to_numpy():\n",
    "#         patches = glob(f\"{tile_dir}/wsi/*\")\n",
    "#         valid_patches += patches\n",
    "#         valid_labels += [label]*len(patches)\n",
    "\n",
    "#     model = timm.create_model(\"tf_efficientnet_b4_ns\", pretrained=True, num_classes=1)\n",
    "\n",
    "#     train_dataset = ToxReprCNNDataset(train_patches, train_labels, None, train_transform)\n",
    "#     valid_dataset = ToxReprCNNDataset(valid_patches, valid_labels, None, valid_transform)\n",
    "\n",
    "#     train_loader = DataLoader(\n",
    "#         dataset=train_dataset,\n",
    "#         num_workers=num_workers,\n",
    "#         batch_size=train_batch_size,\n",
    "#         drop_last=True,\n",
    "#         sampler=ToxReprCNNBalancedSampler(train_dataset, None, np.sum(train_labels)*2),\n",
    "#     )\n",
    "#     valid_loader = DataLoader(\n",
    "#         dataset=valid_dataset,\n",
    "#         num_workers=num_workers,\n",
    "#         batch_size=valid_batch_size,\n",
    "#         shuffle=False,\n",
    "#     )\n",
    "\n",
    "#     criterion = nn.BCEWithLogitsLoss()\n",
    "#     preprocess = lambda x: x.sigmoid()\n",
    "#     optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "#     scheduler = CosineAnnealingLR(optimizer, n_epochs, 5e-7)\n",
    "#     model = model.to(device)\n",
    "#     train_history, valid_history, best_score, best_predictions = train_loop(\n",
    "#         model,\n",
    "#         train_loader,\n",
    "#         valid_loader,\n",
    "#         0,\n",
    "#         criterion,\n",
    "#         optimizer,\n",
    "#         device,\n",
    "#         n_epochs,\n",
    "#         scheduler,\n",
    "#         metrics,\n",
    "#         f\"{save_path}/{ft}\",\n",
    "#         model_name,\n",
    "#         preprocess,\n",
    "#         verbose,\n",
    "#         Logger(),\n",
    "#     )\n",
    "\n",
    "#     with open(os.path.join(save_path, f\"{save_path}/{ft}/train_result.pickle\"), \"wb\") as f:\n",
    "#         pickle.dump((train_history, valid_history, best_score, best_predictions), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
